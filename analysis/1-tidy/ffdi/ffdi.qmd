---
title: Untitled
subtitle: A slightly longer title
format:
  360-analysis-html: default
author: James Goldie
date: last-modified
code-fold: true
---

## (Never mind) Manual FFDI calculation

~~Let's try calculating FFDI using Jacob Arndt's [`kbdi-ffdi`](https://github.com/subond/kbdi-ffdi) Python package.~~

~~We'll use Sale (station 085072) as a test case before we look at anything bigger. It has solid temperature and rainfall records back to the 1950s.~~

~~`kbdi-ffdi` can theoretically also work with NetCDF files, but for CSVs it requires four columns:~~

- ~~`rain` in mm~~
- ~~`temp` in Â°C~~
- ~~`relhum` as a percentage~~
- ~~`wind` in kph~~

## ERA5 FFDI slicing



```{r}
#| label: setup

library(tidyverse)
library(here)
library(sf)
library(terra)
library(exactextractr)
```

Here's the function we'll use to get field averages for a set of `{sf}` polygon `boundaries` over each day of a `raster_path`:

```{r}
#| label: field-avg-fn

calc_field_avgs <- function(raster_path, boundaries) {

  # read grid in with terra
  ffdi_raster <- rast(raster_path)

  # link ffdi layer names to time dimension values
  tibble(
    name = ffdi_raster |> names() |> str_remove("mean."),
    dt = ffdi_raster |> time() |> as_date()) |>
    filter(str_detect(name, "fdimrk")) ->
  layer_dts

  # calculate mean ffdis, then tidy and join to dates
  boundaries |>
    mutate(avg_ffdi = exact_extract(ffdi_raster, boundaries, fun = "mean")) |>
    select(DIST_NO, slug, DIST_NAME, STATE_CODE, avg_ffdi) |>
    st_drop_geometry() |>
    unnest(avg_ffdi) |>
    pivot_longer(-c(DIST_NO, slug, DIST_NAME, STATE_CODE)) |>
    mutate(name = str_remove(name, "mean.")) |>
    filter(str_detect(name, "fdimrk")) |>
    left_join(layer_dts, join_by(name), relationship = "many-to-one")

}
```

```{r}
#| label: run-files

here("data", "aus-firedistricts", "IDM00007.shp") |>
  read_sf() |>
  mutate(slug = janitor::make_clean_names(DIST_NAME)) ->
fire_regions

# calculate ffdi across all the fire regions for each netcdf
here("data", "copernicus-firedanger", "australia") |>
  list.files(pattern = glob2rx("*.nc"), full.names = TRUE) |>
  tibble(path = _) |>
  mutate(ffdi_df = map(path, calc_field_avgs, fire_regions)) ->
ffdi_fireregions

# unnest and write out to disk
ffdi_fireregions |>
  select(ffdi_df) |>
  unnest(ffdi_df) |>
  select(-name) |>
  filter(dt > as_date("1969-06-30")) |>
  write_csv(here("data", "ffdi-fireregions.csv")) ->
ffdi_fireregions_tidy 
```

Let's do some preliminary analysis of a sample area:

```{r}
#| label: visualise

ffdi_fireregions_tidy |>
  filter(DIST_NAME == "New England") |>
  filter(dt < as_date("2023-07-01")) |>
  # filter(month(dt) %in% c(10:12, 1:3)) |>
  mutate(
    dt_mod = dt - months(6),
    year_mod = year(dt_mod)) |>
  group_by(year_mod) |>
  summarise(
    `FFDI average` = mean(value, na.rm = TRUE),
    `FFDI max` = max(value, na.rm = TRUE),
    `FFDI 90th pct` = quantile(value, 0.90)) |>
  ungroup() |>
  pivot_longer(starts_with("FFDI ")) |>
  ggplot() +
    aes(x = year_mod, y = value, colour = name, group = name) +
    geom_line(alpha = 0.5) +
    geom_point(size = 1.5, alpha = 0.75) +
    geom_smooth(method = "lm", se = FALSE) +
  theme_minimal() +
  scale_colour_manual(values = c(
      "FFDI average" = "orange",
      "FFDI 90th pct" = "red",
      "FFDI max" = "black")) +
  theme(
    legend.position = "top",
    legend.direction = "horizontal") +
  labs(x = NULL, y = NULL, name = NULL,
    title = "Forest Fire Danger, July to June",
    subtitle = "New England (NSW)")
```

```{r}
#| label: vis-season

ffdi_fireregions_tidy |>
  filter(DIST_NAME == "New England") |>
  mutate(
    year = year(dt),
    season = ymd(paste("2022", month(dt), mday(dt), sep = "-"))) |>
  ggplot() +
    aes(x = season, y = year, fill = value) +
    geom_tile() +
    scale_x_date(
      labels = scales::label_date(format = "%b"),
      date_breaks = "3 months",
      expand = expansion(0)) +
    scale_y_reverse() +
    scale_fill_stepsn(
      breaks = seq(0, 100, by = 10),
      colours = c(
        "white",
        "white",
        "green",
        "yellow",
        "yellow",
        "orange",
        "orange",
        "red",
        "red",
        "firebrick",
        "firebrick")) +
    theme_minimal() +
    theme(
      legend.direction = "horizontal",
      legend.position = "top") +
    labs(x = NULL, y = NULL,
      title = "Forest Fire Danger",
      subtitle = "New England (NSW)")
  
```

What about frequency? The [2022 State of the Climate](http://www.bom.gov.au/state-of-the-climate/australias-changing-climate.shtml) looked at the number of days exceeding the 90th percentile for a place.

```{r}
#| label: vis-frequency

ffdi_fireregions_tidy |>
  filter(DIST_NAME == "New England") |>
  mutate(
    dt_mod = dt - months(6),
    year_mod = year(dt_mod)) |>
  group_by(DIST_NAME) |>
  mutate(
    pct_90 = quantile(value, 0.9),
    gte_90 = value > pct_90) |>
  filter(gte_90) |>
  group_by(DIST_NAME, year_mod) |>
  summarise(ndays_gte_90pct = n()) |>
  ungroup() |>
  ggplot() +
    aes(x = year_mod, y = ndays_gte_90pct) +
    geom_col() +
    geom_smooth(method = "lm", se = FALSE, colour = "red") +
    theme_minimal() +
    labs(x = NULL, y = NULL,
      title = "Number of days exceeding 95th percentile of FFDI",
      subtitle = "New England (NSW)")
```

Let's do a seasonal take on this:

```{r}
#| label: vis-frequency-seasonal

ffdi_fireregions_tidy |>
  filter(between(dt, as_date("1970-01-01"), as_date("2020-01-01"))) |>
  arrange(dt) |>
  mutate(
    month = factor(month.abb[month(dt)], levels = month.abb),
    year = year(dt),
    decade = factor(paste0(floor(year / 10) * 10, "s")),
    then_or_now = case_match(year,
      1970:1994 ~ "1970_1994",
      1995:2020 ~ "1995_2019")) |>
  # calculate days over 90th percentile
  group_by(DIST_NO, DIST_NAME, slug) |>
  mutate(pct_90 = quantile(value, 0.9)) |>
  ungroup() |>
  group_by(DIST_NO, DIST_NAME, slug, month, then_or_now) |>
  summarise(
    n_gte_90 = sum(value > pct_90, na.rm = TRUE),
    p_gte_90 = n_gte_90 / n()) |>
  ungroup() |>
  glimpse() |>
  pivot_wider(
    id_cols = c(DIST_NO, DIST_NAME, slug, month),
    names_from = then_or_now,
    values_from = c(n_gte_90, p_gte_90)) |>
  write_csv(here("data", "ffdi-firegions-seasonal-thenandnow.csv")) ->
seasonal_highdanger_days

# test the graphic out for new england
ggplot(filter(seasonal_highdanger_days, DIST_NAME == "New England")) +
  aes(x = month) +
  geom_point(
    aes(y = p_gte_90_1970_1994),
    fill = "gold", colour = "black", shape = 21, size = 3.5) +
  geom_point(
    aes(y = p_gte_90_1995_2019),
    fill = "red", colour = "black", shape = 21, size = 3.5) +
  # arrows between then and now 
  geom_segment(
    aes(
      xend = month,
      y = p_gte_90_1970_1994 + 0.015,
      yend = p_gte_90_1995_2019 - 0.015),
    data = filter(seasonal_highdanger_days,
      month %in% month.abb[c(1:4, 10:12)]),
    colour = "orange",
    arrow = grid::arrow(length = unit(0.1, "cm"), type = "closed")
    ) +
  scale_y_continuous(labels = scales::label_percent()) +
  theme_minimal() +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),
  ) +
  labs(
    x = NULL, y = NULL,
    title = "Percentage of days in top 10% of fire danger: 1970-1994 vs. 1995-2019",
    subtitle = "New England (NSW)")
```

This looks great, but it might actually work better as an interactive for distributing across many areas!
